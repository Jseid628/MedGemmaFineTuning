{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Will be evaluating the finetuned model here\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor, PaliGemmaForConditionalGeneration, pipeline\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw has type: <class 'datasets.dataset_dict.DatasetDict'> and has only one entry - raw['train']. (check: len(raw))\n",
    "raw = load_dataset(\"./patchcamelyon_test\")\n",
    "\n",
    "# raw['train'] is of type <class 'datasets.arrow_dataset.Dataset'>, and has 2000 entries (check: len(raw['train']))\n",
    "test_data = raw[\"train\"]\n",
    "\n",
    "# take the first 10 entries for test dataa\n",
    "test_data = test_data.shuffle(seed=42).select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <class 'list'>\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "# options has type <class 'str'>\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "# <class 'str'>\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to think about why this is the structure of the evaluation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add another list to test_data. test_data['messages'] has type <class: 'list'>\n",
    "# elements of test_data['messages'] are lists of len = 1\n",
    "def format_test_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <class 'datasets.arrow_dataset.Dataset'>\n",
    "test_data = test_data.map(format_test_data)\n",
    "\n",
    "# importing accuracy and f1 metrics from evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Ground truth labels\n",
    "# <class 'list'>\n",
    "# REFERENCES[0] has type int\n",
    "REFERENCES = test_data[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the label column to new names:\n",
    "test_data = test_data.cast_column('label', ClassLabel(names=['pos', 'neg']))\n",
    "\n",
    "#print again to check:\n",
    "print(test_data['label'][0])\n",
    "\n",
    "# <class 'datasets.features.features.ClassLabel'> has an int2str method.\n",
    "# test_data.features['label'] is the ClassLabel object which has the labels of the integer classes. \n",
    "# test_data['label'][0] is an integer. And int2str converts that integer into its class label.\n",
    "print(test_data.features['label'].int2str(test_data['label'][0]))\n",
    "\n",
    "# <class 'datasets.features.features.Features'>\n",
    "test_data.features\n",
    "# <class 'datasets.features.image.Image'>\n",
    "test_data.features['image']\n",
    "# <class 'datasets.features.features.ClassLabel'>\n",
    "test_data.features['label']\n",
    "# <class 'list'>\n",
    "test_data.features['messages']\n",
    "\n",
    "# some dataset.Dataset methods\n",
    "# print(test_data.num_columns)\n",
    "# print(test_data.num_rows)\n",
    "# print(test_data.column_names)\n",
    "# print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new dict\n",
    "metrics_dict = {}\n",
    "\n",
    "# updating dict with new data\n",
    "example_data = {'new_data': 42}\n",
    "metrics_dict.update(example_data)\n",
    "print('Updated dict:', metrics_dict)\n",
    "\n",
    "# computing metrics for p against r\n",
    "p = [1, 2, 4]\n",
    "r = [1, 2, 3]\n",
    "metrics_dict.update(accuracy_metric.compute(\n",
    "    predictions = p,\n",
    "    references = r\n",
    "))\n",
    "print(\"Metrics dict with accuracy metric:\", metrics_dict)\n",
    "\n",
    "metrics_dict.update(f1_metric.compute(\n",
    "    predictions = p,\n",
    "    references = r,\n",
    "    average = 'weighted'\n",
    "))\n",
    "print(\"Metrics dict with accuracy and f1 metrics:\", metrics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions: list[int]) -> dict[str, float]:\n",
    "    # <class 'dict'>\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(\n",
    "        # takes ft_predictions which is list[int]\n",
    "        predictions=predictions, # <class: list> (see the function definition)\n",
    "        # REFEFENCES has type list[int]\n",
    "        references=REFERENCES, # <class: list> (definition of REFERENCES)\n",
    "    ))\n",
    "    metrics.update(f1_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=REFERENCES,\n",
    "        average=\"weighted\",\n",
    "    ))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename the class names to the tissue classes, `X: tissue type`\n",
    "test_data = test_data.cast_column(\n",
    "    \"label\",\n",
    "    ClassLabel(names=HISTOPATHOLOGY_CLASSES)\n",
    ")\n",
    "\n",
    "# datasets.features.features.ClassLabel\n",
    "# ground truth labels\n",
    "LABEL_FEATURE = test_data.features[\"label\"]\n",
    "LABEL_FEATURE.str2int('A: no tumor present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in HISTOPATHOLOGY_CLASSES:\n",
    "    print(label)\n",
    "    print(f\"({label.replace(': ', ') ')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping to alternative label format, `(X) tissue type`\n",
    "ALT_LABELS = dict([\n",
    "    (label, f\"({label.replace(': ', ') ')}\") for label in HISTOPATHOLOGY_CLASSES\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do_full_match is set to True\n",
    "def postprocess(prediction: list[dict[str, str]], do_full_match: bool=False) -> int:\n",
    "    response_text = prediction[0][\"generated_text\"]\n",
    "    if do_full_match:\n",
    "        # eg - if response_text = 'A: no tumor present', then will return 0.\n",
    "        return LABEL_FEATURE.str2int(response_text)\n",
    "    for label in HISTOPATHOLOGY_CLASSES:\n",
    "        # Search for `X: tissue type` or `(X) tissue type` in the response\n",
    "        if label in response_text or ALT_LABELS[label] in response_text:\n",
    "            return LABEL_FEATURE.str2int(label)\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Loading Model from Checkpoint ----------- #\n",
    "\n",
    "base_model, processor = utils.load_model_and_processor()\n",
    "lora_check_point_path = './medgemma-4b-it-sft-lora-PatchCamelyon/checkpoint-252'\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, lora_check_point_path)\n",
    "model = model.merge_and_unload()  # Applies the LoRA weights to the original model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Evaluation Pipeline -------- #\n",
    "\n",
    "ft_pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=model,  \n",
    "    processor=processor,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Optional inference tweaks\n",
    "ft_pipe.model.generation_config.do_sample = False\n",
    "ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "# ft_outputs[0] is <class: list> of len = 1\n",
    "# ft_outputs[0][0] is <class 'dict'>\n",
    "# ft_outputs[0][0].keys() >> gives dict_keys(['input_text', 'generated_text'])\n",
    "\n",
    "# <class 'list'>\n",
    "ft_outputs = ft_pipe(\n",
    "    text=test_data[\"messages\"],\n",
    "    images=test_data[\"image\"],\n",
    "    max_new_tokens=20,\n",
    "    batch_size=4,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "# Each one of these ft_outputs[i] gets passed to postprocess(). \n",
    "# The first arguement of ft_outputs[i][0]['generated_text'] is like A: no tumor present\n",
    "for i in range(1):\n",
    "    print(ft_outputs[i][0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ft_predictions = [postprocess(out, do_full_match=True) for out in ft_outputs]\n",
    "\n",
    "ft_metrics = compute_metrics(ft_predictions)\n",
    "print(f\"Fine-tuned metrics: {ft_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
