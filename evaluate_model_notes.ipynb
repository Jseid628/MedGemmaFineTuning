{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/ssd_30T/home/seid/miniconda3/envs/medgemma_train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Will be evaluating the finetuned model here\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor, PaliGemmaForConditionalGeneration, pipeline\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw has type: <class 'datasets.dataset_dict.DatasetDict'> and has only one entry - raw['train']. (check: len(raw))\n",
    "raw = load_dataset(\"./patchcamelyon_test\")\n",
    "\n",
    "# raw['train'] is of type <class 'datasets.arrow_dataset.Dataset'>, and has 2000 entries (check: len(raw['train']))\n",
    "test_data = raw[\"train\"]\n",
    "\n",
    "# take the first 1000 entries for test dataa\n",
    "test_data = test_data.shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <class 'list'>\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "# options has type <class 'str'>\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "# <class 'str'>\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to think about why this is the structure of the evaluation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add another list to test_data. test_data['messages'] has type <class: 'list'>\n",
    "# elements of test_data['messages'] are lists of len = 1\n",
    "def format_test_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# <class 'datasets.arrow_dataset.Dataset'>\n",
    "test_data = test_data.map(format_test_data)\n",
    "\n",
    "print(test_data['label'][0])\n",
    "\n",
    "# <class 'list'>\n",
    "messages_list = test_data.features['messages']\n",
    "\n",
    "# print(test_data.num_columns)\n",
    "# print(test_data.num_rows)\n",
    "# print(test_data.column_names)\n",
    "# print(test_data.shape)\n",
    "\n",
    "\n",
    "# importing accuracy and f1 metrics from evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Ground truth labels\n",
    "# <class 'list'>\n",
    "REFERENCES = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 1000/1000 [00:00<00:00, 27952.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cast the label column to new names:\n",
    "test_data = test_data.cast_column('label', ClassLabel(names=['pos', 'neg']))\n",
    "\n",
    "#print again to check:\n",
    "print(test_data['label'][0])\n",
    "\n",
    "# <class 'datasets.features.features.ClassLabel'> has an int2str method.\n",
    "# test_data.features['label'] is the ClassLabel object which has the labels of the integer classes. \n",
    "# test_data['label'][0] is an integer. And int2str converts that integer into its class label.\n",
    "print(test_data.features['label'].int2str(test_data['label'][0]))\n",
    "\n",
    "# <class 'datasets.features.features.Features'>\n",
    "test_data.features\n",
    "# <class 'datasets.features.features.ClassLabel'>\n",
    "test_data.features['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions: list[int]) -> dict[str, float]:\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=REFERENCES,\n",
    "    ))\n",
    "    metrics.update(f1_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=REFERENCES,\n",
    "        average=\"weighted\",\n",
    "    ))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename the class names to the tissue classes, `X: tissue type`\n",
    "test_data = test_data.cast_column(\n",
    "    \"label\",\n",
    "    ClassLabel(names=HISTOPATHOLOGY_CLASSES)\n",
    ")\n",
    "LABEL_FEATURE = test_data.features[\"label\"]\n",
    "\n",
    "# Mapping to alternative label format, `(X) tissue type`\n",
    "ALT_LABELS = dict([\n",
    "    (label, f\"({label.replace(': ', ') ')}\") for label in HISTOPATHOLOGY_CLASSES\n",
    "])\n",
    "\n",
    "def postprocess(prediction: list[dict[str, str]], do_full_match: bool=False) -> int:\n",
    "    response_text = prediction[0][\"generated_text\"]\n",
    "    if do_full_match:\n",
    "        return LABEL_FEATURE.str2int(response_text)\n",
    "    for label in HISTOPATHOLOGY_CLASSES:\n",
    "        # Search for `X: tissue type` or `(X) tissue type` in the response\n",
    "        if label in response_text or ALT_LABELS[label] in response_text:\n",
    "            return LABEL_FEATURE.str2int(label)\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- Loading Fine Tuned Model ----------- #\n",
    "\n",
    "base_model, processor = utils.load_model_and_processor()\n",
    "\n",
    "lora_check_point_path = './medgemma-4b-it-sft-lora-PatchCamelyon/checkpoint-252'\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, lora_check_point_path)\n",
    "model = model.merge_and_unload()  # Applies the LoRA weights to the original model\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------- Evaluation Pipeline -------- #\n",
    "\n",
    "ft_pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=model,  \n",
    "    processor=processor,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Optional inference tweaks\n",
    "ft_pipe.model.generation_config.do_sample = False\n",
    "ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "ft_outputs = ft_pipe(\n",
    "    text=test_data[\"messages\"],\n",
    "    images=test_data[\"image\"],\n",
    "    max_new_tokens=20,\n",
    "    batch_size=4,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "ft_predictions = [postprocess(out, do_full_match=True) for out in ft_outputs]\n",
    "\n",
    "ft_metrics = compute_metrics(ft_predictions)\n",
    "print(f\"Fine-tuned metrics: {ft_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
