{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "patchcamelyon_subset/\n",
    "├── tumor/\n",
    "│   ├── img00001.png\n",
    "│   ├── img00002.png\n",
    "│   └── ...\n",
    "└── normal/\n",
    "    ├── img00001.png\n",
    "    ├── img00002.png\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other\n",
    "import pandas as pd\n",
    "\n",
    "# For set up\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "# For Loading Model\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# For fine tuning\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of GPUs visible:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Set Up ---------------------- #\n",
    "\n",
    "train_size = 9000 \n",
    "validation_size = 1000\n",
    "\n",
    "# Downloaded and organized a subset of the patchcamelyon data set (first 10K images) into\n",
    "# a folder called patchcamelyon_subset. Has sub folders \"normal\" and \"tumor\"\n",
    "data = load_dataset(\"./patchcamelyon_subset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Dataset({\n",
    "    features: ['image', 'label'],\n",
    "    num_rows: 10000\n",
    "})\n",
    "data['image'][0]:  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7FA624E71B40>\n",
    "data['label'][0]:  0\n",
    "<class 'datasets.arrow_dataset.Dataset'>\n",
    "<class 'list'>\n",
    "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
    "<class 'list'>\n",
    "<class 'int'>\n",
    "```\n",
    "\n",
    "- data is a dataset.Dataset object. \n",
    "- data['image'] and data['label'] are lists\n",
    "- data['image'][0] and data['label][0] is a PIL image and int respectively.\n",
    "- N.B. data[0] **is** a dict object :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=validation_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "# rename the 'test' set to 'validation'\n",
    "data[\"validation\"] = data.pop(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data now has type dataset_dict.DatasetDict:\n",
    "\n",
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "- we can no longer access data[0], since data now is a dictionary - whose two entries in turn are dataset.Dataset objects\n",
    "- data['train'], for example, is:\n",
    "\n",
    "```text\n",
    "Dataset({\n",
    "    features: ['image', 'label'],\n",
    "    num_rows: 9000\n",
    "})\n",
    "```\n",
    "- and data['train'][0] is a dict object, consisting of 'image' and 'label' keys plus their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'example' is the name of the input here - input is a dict.\n",
    "# The key for this dict is a str and the value can be of Any type\n",
    "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    # adds a new entry to the dict\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    # label of 0 will map to: (A: no tumor present), label of 1 will map to: (B: tumor present)\n",
    "                    \"text\": HISTOPATHOLOGY_CLASSES[example[\"label\"]],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    # Returns a dict with the same structure - but now {'image':blah, 'label':hmmm, 'message':blumph}\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_example = format_data(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison showing the effect of format_data() function:\n",
    " \n",
    "- format_data() adds a key (message) and value (the text of the message) to data['train'][0]:\n",
    "\n",
    "```text\n",
    "format_data(data['train'][0]):\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7FA67FC65D50>, 'label': 0, 'messages': [{'role': 'user', 'content': [{'type': 'image'}, {'type': 'text', 'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'A: no tumor present'}]}]}\n",
    "\n",
    "data['train'][1]:\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7FA662E299F0>, 'label': 1}\n",
    "```\n",
    "\n",
    "## Now we format **all** the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(format_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map function gives nice output:\n",
    "```text\n",
    "Map: 100%|██████████| 9000/9000 [00:00<00:00, 15013.28 examples/s]\n",
    "Map: 100%|██████████| 1000/1000 [00:00<00:00, 6887.63 examples/s]\n",
    "```\n",
    "- data is still a dataset_dict.DatasetDict object\n",
    "- data['train'] and data['validation] are still dataset.Dataset objects\n",
    "- data['train'][0] and data['validation][0] are stil dict objects. And now they have three keys: 'image', 'label', 'messages'\n",
    "\n",
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label', 'messages'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label', 'messages'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Loading Model ---------------------- #\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "# major must be 8 to support bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "else: \n",
    "    print('GPU supports bfloat 16. You are good to go :)')\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # optimal device map when using one GPU.\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Add a dictionary entry 'quantization_config' - sets the values of 5 parameters in BitsAndBytesConfig() \n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "# model is assigned the pretrained model (google/medgemma-4b-it) with the specifications (model_kwargs)\n",
    "# ** unpacks the dictionary values as arguments to the from_pretrained function\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# This is where .apply_chat_template looks back to\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up For Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Set Up for Fine Tuning ---------------------- #\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N.B. data['train'][0] is:\n",
    "```text\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7FA663C85EA0>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n",
    "```\n",
    "- That is, a dict with: {'image':value, 'label':value, 'messages':value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has type dict\n",
    "special_tokens = processor.tokenizer.special_tokens_map\n",
    "\n",
    "# Get the first key value pair from special_tokens dict\n",
    "first_key = next(iter(special_tokens))\n",
    "print(first_key, special_tokens[first_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- special_tokens is an object of type dict.\n",
    "- the first key is bos_token, and the first value is bos\n",
    "\n",
    "```text\n",
    "bos_token <bos>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Clone input_ids and assign to labels.\n",
    "# Step 2. Mask unnecessary info\n",
    "# Step 3. Add the now redacted info as a new entry in the batch called 'labels'\n",
    "\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    pixel_values_list = []\n",
    "    token_type_ids_list = []\n",
    "    \n",
    "    for example in examples:\n",
    "        image = example[\"image\"].convert(\"RGB\")\n",
    "        # Applies the chat template from messages and appends that to texts. \n",
    "        # Texts is a list of prompts with both A / B options, and the correct choice A or B.\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).strip()\n",
    "\n",
    "        processed = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Add single processed example lists\n",
    "        input_ids_list.append(processed[\"input_ids\"][0])\n",
    "        attention_mask_list.append(processed[\"attention_mask\"][0])\n",
    "        token_type_ids_list.append(processed['token_type_ids'][0])\n",
    "        pixel_values_list.append(processed[\"pixel_values\"][0])\n",
    "\n",
    "    # Pad sequences - after having added all examples to the lists. Ensures all examples have same length values for given keys.\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "    token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids_list, batch_first=True, padding_value=0)\n",
    "    pixel_values = torch.stack(pixel_values_list)\n",
    "\n",
    "    # ------------------- Label / Masking Step ------------------- #\n",
    "\n",
    "    # We want to predict the text output part of the input. We will later mask the image part.\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Mask special tokens\n",
    "    special_tokens = processor.tokenizer.special_tokens_map\n",
    "    boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([\n",
    "        special_tokens['boi_token'], special_tokens['eoi_token']\n",
    "    ])\n",
    "\n",
    "    # We don't want to predict image values. Any info with image token is masked since part of image.\n",
    "    # Also masking padding tokens / other special tokens.\n",
    "    ignore_token_ids = {\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        boi_token_id,\n",
    "        eoi_token_id,\n",
    "        262144,  # Optional: image token\n",
    "    }\n",
    "\n",
    "    # **Tensor masking** operation for tokens not used in the loss computation.\n",
    "    # 'labels' now contains how we want the model to behave: 'user: Heres an image - is it A or B?'  'model: it is A' All other info masked in labels section of batch.\n",
    "    for token_id in ignore_token_ids:\n",
    "        labels[labels == token_id] = -100\n",
    "\n",
    "    # ------------------------------------------------------------- #\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'lonely_batch is the result of applying collating function to one single input.\n",
    "- In 'lonely_batch' (of type dict), we can see that there is a new input: 'labels'\n",
    "- 'labels' is identical to 'input_ids' but with certain tokens masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data['train'][0]\n",
    "lonely_batch = collate_fn([example])\n",
    "print(type(lonely_batch))\n",
    "print(lonely_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "<class 'dict'>\n",
    "{'input_ids': tensor([[     2,      2,    105,   2364,    109, 255999, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
    "         262144, 256000,    108,   4602,    496,  17491,   1861,    528,    672,\n",
    "           2441, 118234,   2471, 236881,    107, 236776, 236787,    951,  17491,\n",
    "           1861,    107, 236799, 236787,  17491,   1861,    106,    107,    105,\n",
    "           4368,    107, 236776, 236787,    951,  17491,   1861,    106]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]]), 'pixel_values': tensor([[[[ 0.8745,  0.8745,  0.8745,  ...,  0.3255,  0.3255,  0.3255],\n",
    "          [ 0.8745,  0.8745,  0.8745,  ...,  0.3255,  0.3255,  0.3255],\n",
    "          [ 0.8745,  0.8745,  0.8745,  ...,  0.3255,  0.3255,  0.3255],\n",
    "          ...,\n",
    "          [ 0.6863,  0.6863,  0.6863,  ..., -0.0980, -0.0980, -0.0980],\n",
    "          [ 0.6863,  0.6863,  0.6863,  ..., -0.0980, -0.0980, -0.0980],\n",
    "          [ 0.6863,  0.6863,  0.6863,  ..., -0.0980, -0.0980, -0.0980]],\n",
    "\n",
    "         [[ 0.5451,  0.5451,  0.5451,  ..., -0.1216, -0.1216, -0.1216],\n",
    "          [ 0.5451,  0.5451,  0.5451,  ..., -0.1216, -0.1216, -0.1216],\n",
    "          [ 0.5451,  0.5451,  0.5451,  ..., -0.1216, -0.1216, -0.1216],\n",
    "          ...,\n",
    "          [ 0.2392,  0.2392,  0.2392,  ..., -0.3725, -0.3725, -0.3725],\n",
    "          [ 0.2392,  0.2392,  0.2392,  ..., -0.3725, -0.3725, -0.3725],\n",
    "          [ 0.2392,  0.2392,  0.2392,  ..., -0.3725, -0.3725, -0.3725]],\n",
    "\n",
    "         [[ 0.6549,  0.6549,  0.6549,  ...,  0.2000,  0.2000,  0.2000],\n",
    "          [ 0.6549,  0.6549,  0.6549,  ...,  0.2000,  0.2000,  0.2000],\n",
    "          [ 0.6549,  0.6549,  0.6549,  ...,  0.2000,  0.2000,  0.2000],\n",
    "          ...,\n",
    "          [ 0.3882,  0.3882,  0.3882,  ...,  0.1608,  0.1608,  0.1608],\n",
    "          [ 0.3882,  0.3882,  0.3882,  ...,  0.1608,  0.1608,  0.1608],\n",
    "          [ 0.3882,  0.3882,  0.3882,  ...,  0.1608,  0.1608,  0.1608]]]]), 'labels': tensor([[     2,      2,    105,   2364,    109,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,    108,   4602,    496,  17491,   1861,    528,    672,\n",
    "           2441, 118234,   2471, 236881,    107, 236776, 236787,    951,  17491,\n",
    "           1861,    107, 236799, 236787,  17491,   1861,    106,    107,    105,\n",
    "           4368,    107, 236776, 236787,    951,  17491,   1861,    106]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1  # @param {type: \"number\"}\n",
    "learning_rate = 2e-4  # @param {type: \"number\"}\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"medgemma-4b-it-sft-lora-PatchCamelyon\",            # Directory and Hub repository id to save the model to\n",
    "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
    "    per_device_train_batch_size=4,                           # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,                            # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,                           # Number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
    "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
    "    logging_steps=50,                                        # Number of steps between logs\n",
    "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                                           # Number of steps between evaluations\n",
    "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
    "    bf16=True,                                               # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
    "    push_to_hub=False,                                        # Push model to Hub\n",
    "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
    "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
    "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"].shuffle().select(range(200)),  # Use subset of validation set for faster run\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Batch test:\", next(iter(trainer.get_train_dataloader())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.tensor([2, 3, 4])\n",
    "print(my_tensor)\n",
    "\n",
    "# --- Tensor Masking --- #\n",
    "hide_this =  3\n",
    "my_tensor[my_tensor == hide_this] = 0\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first three examples from data['train']\n",
    "data_subset = data['train'].select(range(1))\n",
    "batch = collate_fn(data_subset)\n",
    "#print(f'BATCH: ', batch)\n",
    "\n",
    "# --------------------- Decoding some tokens ----------------------- #\n",
    "\n",
    "# Cannot decode -100 - keep in mind\n",
    "# This is some of the info that remains un-masked mask\n",
    "print(processor.tokenizer.decode([108,   4602,    496,  17491,   1861,    528,    672,\n",
    "           2441, 118234,   2471, 236881,    107, 236776, 236787,    951,  17491,\n",
    "           1861,    107, 236799, 236787,  17491,   1861,    106,    107,    105,\n",
    "           4368,    107, 236799, 236787,  17491,   1861,    106]))\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Flags check if we need to mask more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Need to mask more tokens! ------------------- #\n",
    "special_tokens = processor.tokenizer.special_tokens_map\n",
    "\n",
    "boi_token = special_tokens['boi_token']\n",
    "eoi_token = special_tokens['eoi_token']\n",
    "\n",
    "boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([boi_token, eoi_token])\n",
    "\n",
    "# consider just input_ids\n",
    "input_ids = batch[\"input_ids\"]\n",
    "\n",
    "token_flags = {\n",
    "    'EOI': (input_ids == eoi_token_id).any().item(),\n",
    "    'BOI': (input_ids == boi_token_id).any().item()\n",
    "}\n",
    "\n",
    "for name, found in token_flags.items():\n",
    "    print(f'{name} token found in input ids' if found else f'{name} token not found in input ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "EOI token found in input ids\n",
    "BOI token found in input ids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image token represenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch[\"input_ids\"][0].tolist()\n",
    "num_image_tokens = input_ids.count(262144)\n",
    "print(f\"Number of <image_soft_token> tokens: {num_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Number of <image_soft_token> tokens: 256\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each image: 96 × 96 pixels\n",
    "- Number of image tokens: 256  \n",
    "- Therefore:  \n",
    "\n",
    "  $\\frac{96 \\times 96}{256} = 36 \\text{ pixels per patch} \\Rightarrow \\sqrt{36} = 6 \\times 6 \\text{ pixels per patch}$\n",
    "  \n",
    "- The image is divided into 256 patches, each of size 6×6 pixels.  \n",
    "- These patches are flattened, encoded, and each gets represented by one token (`<image_soft_token>`, token ID 262144).  \n",
    "- So:  \n",
    "\n",
    "  $96 \\times 96 \\text{ image} \\quad \\rightarrow \\quad 256 \\text{ tokens} \\quad (\\text{each representing a 6×6 pixel patch})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods for dataset.Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(input):\n",
    "    return input\n",
    "\n",
    "# Access rows\n",
    "print(data_subset[0])\n",
    "\n",
    "# Select subset of rows\n",
    "data_subset_small = data_subset.select(range(2))\n",
    "\n",
    "# Apply transformation to all rows\n",
    "data_subset_mapped = data_subset.map(my_func)\n",
    "\n",
    "# Split into train/val\n",
    "split_data = data_subset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Shuffle rows\n",
    "shuffled = data_subset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of input_ids[0] is the same as attention_mask[0] within batch\n",
    "if len(batch['input_ids'][0]) == len(batch['attention_mask'][0]):\n",
    "    print('same length')\n",
    "else: print('different length')\n",
    "\n",
    "# Length of each batch['token_type_ids'][i] is 296\n",
    "batch_ttis = batch['token_type_ids']\n",
    "length = len(batch['token_type_ids'])\n",
    "for i in range(length):\n",
    "    print(len(batch_ttis[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "same length\n",
    "```\n",
    "\n",
    "This means that the input_ids and the attention_mask have the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Will be evaluating the finetuned model here\n",
    "\n",
    "from utils import load_model_and_processor\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import evaluate_model\n",
    "\n",
    "# model, processor = load_model_and_processor()\n",
    "# model.eval()\n",
    "\n",
    "raw = load_dataset(\"./patchcamelyon_test\")\n",
    "test_data = raw[\"train\"]\n",
    "test_data = test_data.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n",
    "\n",
    "def format_test_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "test_data = test_data.map(format_test_data)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "REFERENCES = test_data[\"label\"]\n",
    "\n",
    "print(test_data['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions: list[int]) -> dict[str, float]:\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=REFERENCES,\n",
    "    ))\n",
    "    metrics.update(f1_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=REFERENCES,\n",
    "        average=\"weighted\",\n",
    "    ))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "# Rename the class names to the tissue classes, `X: tissue type`\n",
    "test_data = test_data.cast_column(\n",
    "    \"label\",\n",
    "    ClassLabel(names=HISTOPATHOLOGY_CLASSES)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data['label'])\n",
    "LABEL_FEATURE = test_data.features[\"label\"]\n",
    "\n",
    "# Mapping to alternative label format, `(X) tissue type`\n",
    "ALT_LABELS = dict([\n",
    "    (label, f\"({label.replace(': ', ') ')}\") for label in HISTOPATHOLOGY_CLASSES\n",
    "])\n",
    "\n",
    "\n",
    "def postprocess(prediction: list[dict[str, str]], do_full_match: bool=False) -> int:\n",
    "    response_text = prediction[0][\"generated_text\"]\n",
    "    if do_full_match:\n",
    "        return LABEL_FEATURE.str2int(response_text)\n",
    "    for label in HISTOPATHOLOGY_CLASSES:\n",
    "        # Search for `X: tissue type` or `(X) tissue type` in the response\n",
    "        if label in response_text or ALT_LABELS[label] in response_text:\n",
    "            return LABEL_FEATURE.str2int(label)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pt_pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Set `do_sample = False` for deterministic responses\n",
    "pt_pipe.model.generation_config.do_sample = False\n",
    "pt_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pt_outputs = pt_pipe(\n",
    "    text=test_data[\"messages\"],\n",
    "    images=test_data[\"image\"],\n",
    "    max_new_tokens=40,\n",
    "    batch_size=64,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "pt_predictions = [postprocess(out) for out in pt_outputs]\n",
    "\n",
    "\n",
    "pt_metrics = compute_metrics(pt_predictions)\n",
    "print(f\"Baseline metrics: {pt_metrics}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = model_id\n",
    "lora_check_point_path = './medgemma-4b-it-sft-lora-PatchCamelyon/checkpoint-252'\n",
    "\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, lora_check_point_path)\n",
    "model = model.merge_and_unload()  # Applies the LoRA weights to the original model\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)\n",
    "\n",
    "ft_pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=model,  \n",
    "    processor=processor,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Optional inference tweaks\n",
    "ft_pipe.model.generation_config.do_sample = False\n",
    "ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ft_outputs = ft_pipe(\n",
    "    text=test_data[\"messages\"],\n",
    "    images=test_data[\"image\"],\n",
    "    max_new_tokens=20,\n",
    "    batch_size=64,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "ft_predictions = [postprocess(out, do_full_match=True) for out in ft_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned metrics: {'accuracy': 0.869, 'f1': 0.86852804562022}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ft_metrics = compute_metrics(ft_predictions)\n",
    "print(f\"Fine-tuned metrics: {ft_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
