{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "patchcamelyon_subset/\n",
    "├── tumor/\n",
    "│   ├── img00001.png\n",
    "│   ├── img00002.png\n",
    "│   └── ...\n",
    "└── normal/\n",
    "    ├── img00001.png\n",
    "    ├── img00002.png\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other\n",
    "import pandas as pd\n",
    "\n",
    "# For set up\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "# For Loading Model\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# For fine tuning\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of GPUs visible:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Set Up ---------------------- #\n",
    "\n",
    "train_size = 9000 \n",
    "validation_size = 1000 \n",
    "\n",
    "# Downloaded and organized a subset of the patchcamelyon data set (first 10K images) into\n",
    "# a folder called patchcamelyon_subset. Has sub folders \"normal\" and \"tumor\"\n",
    "data = load_dataset(\"./patchcamelyon_subset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Dataset({\n",
    "    features: ['image', 'label'],\n",
    "    num_rows: 10000\n",
    "})\n",
    "data['image'][0]:  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7FA624E71B40>\n",
    "data['label'][0]:  0\n",
    "<class 'datasets.arrow_dataset.Dataset'>\n",
    "<class 'list'>\n",
    "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
    "<class 'list'>\n",
    "<class 'int'>\n",
    "```\n",
    "\n",
    "- data is a dataset.Dataset object. \n",
    "- data['image'] and data['label'] are lists\n",
    "- data['image'][0] and data['label][0] is a PIL image and int respectively.\n",
    "- N.B. data[0] **is** a dict object :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(\n\u001b[1;32m      2\u001b[0m     train_size\u001b[38;5;241m=\u001b[39mtrain_size,\n\u001b[1;32m      3\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mvalidation_size,\n\u001b[1;32m      4\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# rename the 'test' set to 'validation'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = data.train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=validation_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "# rename the 'test' set to 'validation'\n",
    "data[\"validation\"] = data.pop(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data now has type dataset_dict.DatasetDict:\n",
    "\n",
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "- we can no longer access data[0], since data now is a dictionary - whose two entries in turn are dataset.Dataset objects\n",
    "- data['train'], for example, is:\n",
    "\n",
    "```text\n",
    "Dataset({\n",
    "    features: ['image', 'label'],\n",
    "    num_rows: 9000\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: no tumor present\n",
      "B: tumor present\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n",
    "\n",
    "# 'example' is the name of the input here - input is a dict.\n",
    "# The key for this dict is a str and the value can be of Any type\n",
    "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    # adds a new entry to the dict\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    # label of 0 will map to: (A: no tumor present), label of 1 will map to: (B: tumor present)\n",
    "                    \"text\": HISTOPATHOLOGY_CLASSES[example[\"label\"]],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    # Returns a dict with the same structure - but now {'image':blah, 'label':hmmm, 'message':blumph}\n",
    "    return example\n",
    "\n",
    "data = data.map(format_data)\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "data['train'][0]: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA3E0>, 'label': 0}\n",
    "Image saved to sample_image.png\n",
    "0\n",
    "ClassLabel(names=['normal', 'tumor'], id=None)\n",
    "Map: 100%|██████████| 9000/9000 [00:00<00:00, 10200.15 examples/s]\n",
    "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5657.86 examples/s]\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA980>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Loading Model ---------------------- #\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "else: \n",
    "    print('GPU supports bfloat 16. You are good to go :)')\n",
    "\n",
    "# A dictionary of model arguments - ie, 'attn_implementation' maps to 'eager'\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Add a dictionary entry 'quantization_config' - sets the values of 5 parameters in BitsAndBytesConfig() \n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# This is where .apply_chat_template looks back to\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up For Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Set Up for Fine Tuning ---------------------- #\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    pixel_values_list = []\n",
    "    token_type_ids_list = []\n",
    "    \n",
    "    for example in examples:\n",
    "        image = example[\"image\"].convert(\"RGB\")\n",
    "        # Applies the chat template from messages and appends that to texts. \n",
    "        # Texts is a list of prompts with both A / B options, and the correct choice A or B.\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).strip()\n",
    "\n",
    "        processed = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Add single processed example lists\n",
    "        input_ids_list.append(processed[\"input_ids\"][0])\n",
    "        attention_mask_list.append(processed[\"attention_mask\"][0])\n",
    "        token_type_ids_list.append(processed['token_type_ids'][0])\n",
    "        pixel_values_list.append(processed[\"pixel_values\"][0])\n",
    "\n",
    "    # Pad sequences - after having added all examples. Ensures all examples have same length values for given keys.\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "    print(\"input_ids: \\n\", input_ids)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "    token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids_list, batch_first=True, padding_value=0)\n",
    "    pixel_values = torch.stack(pixel_values_list)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Mask special tokens\n",
    "    special_tokens = processor.tokenizer.special_tokens_map\n",
    "    boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([\n",
    "        special_tokens['boi_token'], special_tokens['eoi_token']\n",
    "    ])\n",
    "    ignore_token_ids = {\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        boi_token_id,\n",
    "        eoi_token_id,\n",
    "        262144,  # Optional: image token\n",
    "    }\n",
    "\n",
    "    for token_id in ignore_token_ids:\n",
    "        labels[labels == token_id] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data['train'][0]\n",
    "lonely_batch = collate_fn([example])\n",
    "print(lonely_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1. Clone input_ids and assign to labels.\n",
    "# Step 2. Mask unnecessary info\n",
    "# Step 3. Add the now redacted info as a new entry in the batch called 'labels'\n",
    "\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        # Applies the chat template from messages and appends that to texts. \n",
    "        # Texts is a list of prompts with both A / B options, and the correct choice A or B.\n",
    "        texts.append(processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).strip())\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    # Contains 'input_ids'\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # These labels are tokenized version of the input\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Masking step begins here\n",
    "    special_tokens = processor.tokenizer.special_tokens_map\n",
    "\n",
    "    boi_token = special_tokens['boi_token']\n",
    "    eoi_token = special_tokens['eoi_token']\n",
    "\n",
    "    boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([boi_token, eoi_token])\n",
    "\n",
    "    # We don't want to predict image values. Any info with image token is masked since part of image.\n",
    "    # Also masking padding tokens / other special tokens.\n",
    "    ignore_token_ids = {\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        boi_token_id,\n",
    "        eoi_token_id,\n",
    "        262144\n",
    "    }\n",
    "\n",
    "    # **Tensor masking** operation for tokens not used in the loss computation.\n",
    "    for token_id in ignore_token_ids:\n",
    "        labels[labels == token_id] = -100\n",
    "\n",
    "    # 'labels' now contains how we want the model to behave: 'user: Heres an image - is it A or B?'  'model: it is A' All other info masked in labels section of batch.\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1  # @param {type: \"number\"}\n",
    "learning_rate = 2e-4  # @param {type: \"number\"}\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"medgemma-4b-it-sft-lora-PatchCamelyon\",            # Directory and Hub repository id to save the model to\n",
    "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
    "    per_device_train_batch_size=4,                           # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,                            # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,                           # Number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
    "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
    "    logging_steps=50,                                        # Number of steps between logs\n",
    "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                                           # Number of steps between evaluations\n",
    "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
    "    bf16=True,                                               # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
    "    push_to_hub=False,                                        # Push model to Hub\n",
    "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
    "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
    "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"].shuffle().select(range(200)),  # Use subset of validation set for faster run\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Batch test:\", next(iter(trainer.get_train_dataloader())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.tensor([2, 3, 4])\n",
    "print(my_tensor)\n",
    "\n",
    "# --- Tensor Masking --- #\n",
    "hide_this =  3\n",
    "my_tensor[my_tensor == hide_this] = 0\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first three examples from data['train']\n",
    "data_subset = data['train'].select(range(1))\n",
    "batch = collate_fn(data_subset)\n",
    "#print(f'BATCH: ', batch)\n",
    "\n",
    "# --------------------- Decoding some tokens ----------------------- #\n",
    "\n",
    "# Cannot decode -100 - keep in mind\n",
    "# This is some of the info that remains un-masked mask\n",
    "print(processor.tokenizer.decode([108,   4602,    496,  17491,   1861,    528,    672,\n",
    "           2441, 118234,   2471, 236881,    107, 236776, 236787,    951,  17491,\n",
    "           1861,    107, 236799, 236787,  17491,   1861,    106,    107,    105,\n",
    "           4368,    107, 236799, 236787,  17491,   1861,    106]))\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Flags check if we need to mask more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Need to mask more tokens! ------------------- #\n",
    "special_tokens = processor.tokenizer.special_tokens_map\n",
    "\n",
    "boi_token = special_tokens['boi_token']\n",
    "eoi_token = special_tokens['eoi_token']\n",
    "\n",
    "boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([boi_token, eoi_token])\n",
    "\n",
    "# consider just input_ids\n",
    "input_ids = batch[\"input_ids\"]\n",
    "\n",
    "token_flags = {\n",
    "    'EOI': (input_ids == eoi_token_id).any().item(),\n",
    "    'BOI': (input_ids == boi_token_id).any().item()\n",
    "}\n",
    "\n",
    "for name, found in token_flags.items():\n",
    "    print(f'{name} token found in input ids' if found else f'{name} token not found in input ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "EOI token found in input ids\n",
    "BOI token found in input ids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image token represenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch[\"input_ids\"][0].tolist()\n",
    "num_image_tokens = input_ids.count(262144)\n",
    "print(f\"Number of <image_soft_token> tokens: {num_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Number of <image_soft_token> tokens: 256\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each image: 96 × 96 pixels\n",
    "- Number of image tokens: 256  \n",
    "- Therefore:  \n",
    "\n",
    "  $\\frac{96 \\times 96}{256} = 36 \\text{ pixels per patch} \\Rightarrow \\sqrt{36} = 6 \\times 6 \\text{ pixels per patch}$\n",
    "  \n",
    "- The image is divided into 256 patches, each of size 6×6 pixels.  \n",
    "- These patches are flattened, encoded, and each gets represented by one token (`<image_soft_token>`, token ID 262144).  \n",
    "- So:  \n",
    "\n",
    "  $96 \\times 96 \\text{ image} \\quad \\rightarrow \\quad 256 \\text{ tokens} \\quad (\\text{each representing a 6×6 pixel patch})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods for dataset.Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(input):\n",
    "    return input\n",
    "\n",
    "# Access rows\n",
    "print(data_subset[0])\n",
    "\n",
    "# Select subset of rows\n",
    "data_subset_small = data_subset.select(range(2))\n",
    "\n",
    "# Apply transformation to all rows\n",
    "data_subset_mapped = data_subset.map(my_func)\n",
    "\n",
    "# Split into train/val\n",
    "split_data = data_subset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Shuffle rows\n",
    "shuffled = data_subset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of input_ids[0] is the same as attention_mask[0] within batch\n",
    "if len(batch['input_ids'][0]) == len(batch['attention_mask'][0]):\n",
    "    print('same length')\n",
    "else: print('different length')\n",
    "\n",
    "# Length of each batch['token_type_ids'][i] is 296\n",
    "batch_ttis = batch['token_type_ids']\n",
    "length = len(batch['token_type_ids'])\n",
    "for i in range(length):\n",
    "    print(len(batch_ttis[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "same length\n",
    "```\n",
    "\n",
    "This means that the input_ids and the attention_mask have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- scratch work ------------- #\n",
    "# tensor_a = torch.rand(5)\n",
    "# print(tensor_a)\n",
    "# print(len(tensor_a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
