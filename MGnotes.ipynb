{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "patchcamelyon_subset/\n",
    "├── tumor/\n",
    "│   ├── img00001.png\n",
    "│   ├── img00002.png\n",
    "│   └── ...\n",
    "└── normal/\n",
    "    ├── img00001.png\n",
    "    ├── img00002.png\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/ssd_30T/home/seid/miniconda3/envs/medgemma_train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Other\n",
    "import pandas as pd\n",
    "\n",
    "# For set up\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "# For Loading Model\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# For fine tuning\n",
    "from peft import LoraConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs visible: 1\n",
      "Current device: 0\n",
      "GPU name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GPUs visible:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Set Up ---------------------- #\n",
    "\n",
    "train_size = 9000 \n",
    "validation_size = 1000 \n",
    "\n",
    "# Downloaded and organized a subset of the patchcamelyon data set (first 10K images) into\n",
    "# a folder called patchcamelyon_subset. Has sub folders \"normal\" and \"tumor\"\n",
    "data = load_dataset(\"./patchcamelyon_subset\", split=\"train\")\n",
    "data = data.train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=validation_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "# rename the 'test' set to 'validation'\n",
    "data[\"validation\"] = data.pop(\"test\")\n",
    "\n",
    "# ------------ Optional: display dataset details ------------ #\n",
    "print(data) \n",
    "# This is actually a dictionary - it contains {'image':blah, 'label':hmmm}\n",
    "print(f\"data['train'][0]: {data['train'][0]}\")\n",
    "# First image in the training data\n",
    "image = data['train'][0]['image']\n",
    "# First label in the training data\n",
    "label = data['train'][0]['label']\n",
    "image.save(\"sample_image.png\")\n",
    "print(\"Image saved to sample_image.png\")\n",
    "print(label)\n",
    "print(data['train'].features['label'])\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n",
    "\n",
    "# 'example' is the name of the input here - input is a dict.\n",
    "# The key for this dict is a str and the value can be of Any type\n",
    "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    # adds a new entry to the dict\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    # label of 0 will map to: (A: no tumor present), label of 1 will map to: (B: tumor present)\n",
    "                    \"text\": HISTOPATHOLOGY_CLASSES[example[\"label\"]],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    # Returns a dict with the same structure - but now {'image':blah, 'label':hmmm, 'message':blumph}\n",
    "    return example\n",
    "\n",
    "data = data.map(format_data)\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "data['train'][0]: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA3E0>, 'label': 0}\n",
    "Image saved to sample_image.png\n",
    "0\n",
    "ClassLabel(names=['normal', 'tumor'], id=None)\n",
    "Map: 100%|██████████| 9000/9000 [00:00<00:00, 10200.15 examples/s]\n",
    "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5657.86 examples/s]\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA980>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Loading Model ---------------------- #\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "else: \n",
    "    print('GPU supports bfloat 16. You are good to go :)')\n",
    "\n",
    "# A dictionary of model arguments - ie, 'attn_implementation' maps to 'eager'\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Add a dictionary entry 'quantization_config' - sets the values of 5 parameters in BitsAndBytesConfig() \n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# This is where .apply_chat_template looks back to\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up For Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Set Up for Fine Tuning ---------------------- #\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1. Clone input_ids and assign to labels.\n",
    "# Step 2. Mask unnecessary info\n",
    "# Step 3. Add the now redacted info as a new entry in the batch called 'labels'\n",
    "\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        # Applies the chat template from messages and appends that to texts. \n",
    "        # Texts is a list of prompts with both A / B options, and the correct choice A or B.\n",
    "        texts.append(processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).strip())\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    # Contains 'input_ids'\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # These labels are tokenized version of the input\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask boi_token (255999). (B)eggining (O)f (I)mage token\n",
    "    boi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "\n",
    "    # **Tensor masking** operation for tokens not used in the loss computation.\n",
    "    # For instance, we don't want to predict image values. Any info with image token is masked since part of image.\n",
    "    # Also masking padding tokens / other special tokens.\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == boi_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    # 'labels' contains how we want the model to behave: \"Heres an image - is it A or B? A\" for example. All other info masked.\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([2, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.tensor([2, 3, 4])\n",
    "print(my_tensor)\n",
    "\n",
    "# --- Tensor Masking --- #\n",
    "hide_this =  3\n",
    "my_tensor[my_tensor == hide_this] = 0\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Is a tumor present in this histopathology image?\n",
      "A: no tumor present\n",
      "B: tumor present<end_of_turn>\n",
      "<start_of_turn>model\n",
      "B: tumor present<end_of_turn>\n",
      "\n",
      "\n",
      "<eoi> token is present in input_ids.\n"
     ]
    }
   ],
   "source": [
    "# Take first three examples from data['train']\n",
    "data_subset = data['train'].select(range(3))\n",
    "batch = collate_fn(data_subset)\n",
    "#print(f'BATCH: ', batch)\n",
    "\n",
    "# --------------------- Decoding some tokens ----------------------- #\n",
    "\n",
    "# Cannot decode -100 - keep in mind\n",
    "# This is some of the info that remains un-masked mask\n",
    "print(processor.tokenizer.decode([108,   4602,    496,  17491,   1861,    528,    672,\n",
    "           2441, 118234,   2471, 236881,    107, 236776, 236787,    951,  17491,\n",
    "           1861,    107, 236799, 236787,  17491,   1861,    106,    107,    105,\n",
    "           4368,    107, 236799, 236787,  17491,   1861,    106]))\n",
    "print('\\n')\n",
    "# -------------------- Need to mask more tokens? ------------------- #\n",
    "\n",
    "# consider just input_ids\n",
    "input_ids = batch[\"input_ids\"]\n",
    "has_eoi = (input_ids == eoi_token_id).any().item()\n",
    "\n",
    "if has_eoi:\n",
    "    print(\"<eoi> token is present in input_ids.\")\n",
    "else:\n",
    "    print(\"<eoi> token is NOT present in input_ids.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "<class 'datasets.arrow_dataset.Dataset'>\n",
    "<bos><bos><start_of_turn>user\n",
    "\n",
    "\n",
    "<start_of_image><image_soft_token><image_soft_token>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch[\"input_ids\"][0].tolist()\n",
    "num_image_tokens = input_ids.count(262144)\n",
    "print(f\"Number of <image_soft_token> tokens: {num_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Number of <image_soft_token> tokens: 256\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each image: 96 × 96 pixels\n",
    "- Number of image tokens: 256  \n",
    "- Therefore:  \n",
    "\n",
    "  $\\frac{96 \\times 96}{256} = 36 \\text{ pixels per patch} \\Rightarrow \\sqrt{36} = 6 \\times 6 \\text{ pixels per patch}$\n",
    "  \n",
    "- The image is divided into 256 patches, each of size 6×6 pixels.  \n",
    "- These patches are flattened, encoded, and each gets represented by one token (`<image_soft_token>`, token ID 262144).  \n",
    "- So:  \n",
    "\n",
    "  $96 \\times 96 \\text{ image} \\quad \\rightarrow \\quad 256 \\text{ tokens} \\quad (\\text{each representing a 6×6 pixel patch})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finding the length of each batch['token_type_ids'][i] \n",
    "batch_ttis = batch['token_type_ids']\n",
    "length = len(batch['token_type_ids'])\n",
    "for i in range(length):\n",
    "    print(len(batch_ttis[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOI token ID: 256000\n"
     ]
    }
   ],
   "source": [
    "eoi_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "    processor.tokenizer.special_tokens_map[\"eoi_token\"]\n",
    ")\n",
    "print(\"EOI token ID:\", eoi_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "<start_of_image>\n",
      "255999\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.special_tokens_map)\n",
    "print(processor.tokenizer.special_tokens_map['boi_token'])\n",
    "print(processor.tokenizer.convert_tokens_to_ids('<start_of_image>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods for dataset.Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(input):\n",
    "    return input\n",
    "\n",
    "# Access rows\n",
    "print(data_subset[0])\n",
    "\n",
    "# Select subset of rows\n",
    "data_subset_small = data_subset.select(range(2))\n",
    "\n",
    "# Apply transformation to all rows\n",
    "data_subset_mapped = data_subset.map(my_func)\n",
    "\n",
    "# Split into train/val\n",
    "split_data = data_subset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Shuffle rows\n",
    "shuffled = data_subset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(batch['input_ids'][0]) == len(batch['attention_mask'][0]):\n",
    "    print('same length')\n",
    "else: print('different length')\n",
    "\n",
    "print(len(batch['input_ids'][0]))\n",
    "\n",
    "print(len(batch['token_type_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "same length\n",
    "```\n",
    "\n",
    "This means that the input_ids and the attention_mask have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- scratch work ------------- #\n",
    "# tensor_a = torch.rand(5)\n",
    "# print(tensor_a)\n",
    "# print(len(tensor_a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
