{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "patchcamelyon_subset/\n",
    "├── tumor/\n",
    "│   ├── img00001.png\n",
    "│   ├── img00002.png\n",
    "│   └── ...\n",
    "└── normal/\n",
    "    ├── img00001.png\n",
    "    ├── img00002.png\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other\n",
    "import pandas as pd\n",
    "\n",
    "# For set up\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "# For Loading Model\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# For fine tuning\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs visible: 1\n",
      "Current device: 0\n",
      "GPU name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GPUs visible:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "data['train'][0]: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F75A685ECE0>, 'label': 0}\n",
      "Image saved to sample_image.png\n",
      "0\n",
      "ClassLabel(names=['normal', 'tumor'], id=None)\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F75A685C490>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------- Set Up ---------------------- #\n",
    "\n",
    "train_size = 9000 \n",
    "validation_size = 1000 \n",
    "\n",
    "# Downloaded and organized a subset of the patchcamelyon data set (first 10K images) into\n",
    "# a folder called patchcamelyon_subset. Has sub folders \"normal\" and \"tumor\"\n",
    "data = load_dataset(\"./patchcamelyon_subset\", split=\"train\")\n",
    "data = data.train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=validation_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "# rename the 'test' set to 'validation'\n",
    "data[\"validation\"] = data.pop(\"test\")\n",
    "\n",
    "# ------------ Optional: display dataset details ------------ #\n",
    "print(data) \n",
    "# This is actually a dictionary - it contains {'image':blah, 'label':hmmm}\n",
    "print(f\"data['train'][0]: {data['train'][0]}\")\n",
    "# First image in the training data\n",
    "image = data['train'][0]['image']\n",
    "# First label in the training data\n",
    "label = data['train'][0]['label']\n",
    "image.save(\"sample_image.png\")\n",
    "print(\"Image saved to sample_image.png\")\n",
    "print(label)\n",
    "print(data['train'].features['label'])\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n",
    "\n",
    "# 'example' is the name of the input here - input is a dict.\n",
    "# The key for this dict is a str and the value can be of Any type\n",
    "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    # adds a new entry to the dict\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    # label of 0 will map to: (A: no tumor present), label of 1 will map to: (B: tumor present)\n",
    "                    \"text\": HISTOPATHOLOGY_CLASSES[example[\"label\"]],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    # Returns a dict with the same structure - but now {'image':blah, 'label':hmmm, 'message':blumph}\n",
    "    return example\n",
    "\n",
    "data = data.map(format_data)\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "data['train'][0]: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA3E0>, 'label': 0}\n",
    "Image saved to sample_image.png\n",
    "0\n",
    "ClassLabel(names=['normal', 'tumor'], id=None)\n",
    "Map: 100%|██████████| 9000/9000 [00:00<00:00, 10200.15 examples/s]\n",
    "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5657.86 examples/s]\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA980>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU supports bfloat 16. You are good to go :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Loading Model ---------------------- #\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "else: \n",
    "    print('GPU supports bfloat 16. You are good to go :)')\n",
    "\n",
    "# A dictionary of model arguments - ie, 'attn_implementation' maps to 'eager'\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Add a dictionary entry 'quantization_config' - sets the values of 5 parameters in BitsAndBytesConfig() \n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# This is where .apply_chat_template looks back to\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up For Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Set Up for Fine Tuning ---------------------- #\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1. Clone input_ids and assign to labels.\n",
    "# Step 2. Mask unnecessary info\n",
    "# Step 3. Add the now redacted info as a new entry in the batch called 'labels'\n",
    "\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        # Applies the chat template from messages and appends that to texts. \n",
    "        # Texts is a list of prompts with both A / B options, and the correct choice A or B.\n",
    "        texts.append(processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).strip())\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    # Contains 'input_ids'\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # These labels are tokenized version of the input\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Masking step begins here\n",
    "    special_tokens = processor.tokenizer.special_tokens_map\n",
    "\n",
    "    boi_token = special_tokens['boi_token']\n",
    "    eoi_token = special_tokens['eoi_token']\n",
    "\n",
    "    boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([boi_token, eoi_token])\n",
    "\n",
    "    # We don't want to predict image values. Any info with image token is masked since part of image.\n",
    "    # Also masking padding tokens / other special tokens.\n",
    "    ignore_token_ids = {\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        boi_token_id,\n",
    "        eoi_token_id,\n",
    "        262144\n",
    "    }\n",
    "\n",
    "    # **Tensor masking** operation for tokens not used in the loss computation.\n",
    "    for token_id in ignore_token_ids:\n",
    "        labels[labels == token_id] = -100\n",
    "\n",
    "    # 'labels' now contains how we want the model to behave: 'user: Heres an image - is it A or B?'  'model: it is A' All other info masked in labels section of batch.\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 47.53 GiB of which 196.12 MiB is free. Process 3743819 has 37.04 GiB memory in use. Including non-PyTorch memory, this process has 10.28 GiB memory in use. Of the allocated memory 9.89 GiB is allocated by PyTorch, and 93.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 29\u001b[0m\n\u001b[1;32m      2\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-4\u001b[39m  \u001b[38;5;66;03m# @param {type: \"number\"}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedgemma-4b-it-sft-lora-PatchCamelyon\u001b[39m\u001b[38;5;124m\"\u001b[39m,            \u001b[38;5;66;03m# Directory and Hub repository id to save the model to\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mnum_train_epochs,                       \u001b[38;5;66;03m# Number of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     label_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],                                  \u001b[38;5;66;03m# Input keys that correspond to the labels\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use subset of validation set for faster run\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch test:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mget_train_dataloader())))\n",
      "File \u001b[0;32m~/miniconda3/envs/medgemma_train/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:397\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# PEFT configuration and model wrapping\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Data collator\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# FFD packing requires padding-free mode; otherwise, the collator outputs padded attention masks, causing\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# FlashAttention to ignore position_ids and recompute them incorrectly from the padded attention mask.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_free \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpadding_free \u001b[38;5;129;01mor\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mpacking \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mpacking_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/medgemma_train/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:581\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_peft_model\u001b[0;34m(self, model, peft_config, args)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Prepare model for kbit training if needed\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_qlora \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded_qlora:\n\u001b[0;32m--> 581\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# Disable gradient checkpointing as it's handled by prepare_model_for_kbit_training\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     args \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(args, gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/medgemma_train/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:610\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_model_for_kbit_training\u001b[0;34m(self, model, args)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares a quantized model for kbit training.\"\"\"\u001b[39;00m\n\u001b[1;32m    605\u001b[0m prepare_model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mgradient_checkpointing,\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_checkpointing_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mgradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    608\u001b[0m }\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_model_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medgemma_train/lib/python3.10/site-packages/peft/utils/other.py:149\u001b[0m, in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    147\u001b[0m             (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m    148\u001b[0m         ) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 149\u001b[0m             param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    152\u001b[0m     loaded_in_kbit\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_gptq_quantized\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m gradient_checkpointing_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 47.53 GiB of which 196.12 MiB is free. Process 3743819 has 37.04 GiB memory in use. Including non-PyTorch memory, this process has 10.28 GiB memory in use. Of the allocated memory 9.89 GiB is allocated by PyTorch, and 93.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_train_epochs = 1  # @param {type: \"number\"}\n",
    "learning_rate = 2e-4  # @param {type: \"number\"}\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"medgemma-4b-it-sft-lora-PatchCamelyon\",            # Directory and Hub repository id to save the model to\n",
    "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
    "    per_device_train_batch_size=4,                           # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,                            # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=4,                           # Number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
    "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
    "    logging_steps=50,                                        # Number of steps between logs\n",
    "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
    "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                                           # Number of steps between evaluations\n",
    "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
    "    bf16=True,                                               # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
    "    push_to_hub=False,                                        # Push model to Hub\n",
    "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
    "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
    "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"].shuffle().select(range(200)),  # Use subset of validation set for faster run\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Batch test:\", next(iter(trainer.get_train_dataloader())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([2, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.tensor([2, 3, 4])\n",
    "print(my_tensor)\n",
    "\n",
    "# --- Tensor Masking --- #\n",
    "hide_this =  3\n",
    "my_tensor[my_tensor == hide_this] = 0\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Is a tumor present in this histopathology image?\n",
      "A: no tumor present\n",
      "B: tumor present<end_of_turn>\n",
      "<start_of_turn>model\n",
      "B: tumor present<end_of_turn>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take first three examples from data['train']\n",
    "data_subset = data['train'].select(range(3))\n",
    "batch = collate_fn(data_subset)\n",
    "#print(f'BATCH: ', batch)\n",
    "\n",
    "# --------------------- Decoding some tokens ----------------------- #\n",
    "\n",
    "# Cannot decode -100 - keep in mind\n",
    "# This is some of the info that remains un-masked mask\n",
    "print(processor.tokenizer.decode([108,   4602,    496,  17491,   1861,    528,    672,\n",
    "           2441, 118234,   2471, 236881,    107, 236776, 236787,    951,  17491,\n",
    "           1861,    107, 236799, 236787,  17491,   1861,    106,    107,    105,\n",
    "           4368,    107, 236799, 236787,  17491,   1861,    106]))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Flags check if we need to mask more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOI token found in input ids\n",
      "BOI token found in input ids\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Need to mask more tokens! ------------------- #\n",
    "special_tokens = processor.tokenizer.special_tokens_map\n",
    "\n",
    "boi_token = special_tokens['boi_token']\n",
    "eoi_token = special_tokens['eoi_token']\n",
    "\n",
    "boi_token_id, eoi_token_id = processor.tokenizer.convert_tokens_to_ids([boi_token, eoi_token])\n",
    "\n",
    "# consider just input_ids\n",
    "input_ids = batch[\"input_ids\"]\n",
    "\n",
    "token_flags = {\n",
    "    'EOI': (input_ids == eoi_token_id).any().item(),\n",
    "    'BOI': (input_ids == boi_token_id).any().item()\n",
    "}\n",
    "\n",
    "for name, found in token_flags.items():\n",
    "    print(f'{name} token found in input ids' if found else f'{name} token not found in input ids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "EOI token found in input ids\n",
    "BOI token found in input ids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image token represenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <image_soft_token> tokens: 256\n"
     ]
    }
   ],
   "source": [
    "input_ids = batch[\"input_ids\"][0].tolist()\n",
    "num_image_tokens = input_ids.count(262144)\n",
    "print(f\"Number of <image_soft_token> tokens: {num_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Number of <image_soft_token> tokens: 256\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each image: 96 × 96 pixels\n",
    "- Number of image tokens: 256  \n",
    "- Therefore:  \n",
    "\n",
    "  $\\frac{96 \\times 96}{256} = 36 \\text{ pixels per patch} \\Rightarrow \\sqrt{36} = 6 \\times 6 \\text{ pixels per patch}$\n",
    "  \n",
    "- The image is divided into 256 patches, each of size 6×6 pixels.  \n",
    "- These patches are flattened, encoded, and each gets represented by one token (`<image_soft_token>`, token ID 262144).  \n",
    "- So:  \n",
    "\n",
    "  $96 \\times 96 \\text{ image} \\quad \\rightarrow \\quad 256 \\text{ tokens} \\quad (\\text{each representing a 6×6 pixel patch})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods for dataset.Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F777CEF4040>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 30.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def my_func(input):\n",
    "    return input\n",
    "\n",
    "# Access rows\n",
    "print(data_subset[0])\n",
    "\n",
    "# Select subset of rows\n",
    "data_subset_small = data_subset.select(range(2))\n",
    "\n",
    "# Apply transformation to all rows\n",
    "data_subset_mapped = data_subset.map(my_func)\n",
    "\n",
    "# Split into train/val\n",
    "split_data = data_subset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Shuffle rows\n",
    "shuffled = data_subset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same length\n",
      "296\n",
      "296\n",
      "296\n"
     ]
    }
   ],
   "source": [
    "# Length of input_ids[0] is the same as attention_mask[0] within batch\n",
    "if len(batch['input_ids'][0]) == len(batch['attention_mask'][0]):\n",
    "    print('same length')\n",
    "else: print('different length')\n",
    "\n",
    "# Length of each batch['token_type_ids'][i] is 296\n",
    "batch_ttis = batch['token_type_ids']\n",
    "length = len(batch['token_type_ids'])\n",
    "for i in range(length):\n",
    "    print(len(batch_ttis[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "same length\n",
    "```\n",
    "\n",
    "This means that the input_ids and the attention_mask have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- scratch work ------------- #\n",
    "# tensor_a = torch.rand(5)\n",
    "# print(tensor_a)\n",
    "# print(len(tensor_a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
