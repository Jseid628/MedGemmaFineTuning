{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "patchcamelyon_subset/\n",
    "├── tumor/\n",
    "│   ├── img00001.png\n",
    "│   ├── img00002.png\n",
    "│   └── ...\n",
    "└── normal/\n",
    "    ├── img00001.png\n",
    "    ├── img00002.png\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For set up\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "# For Loading Model\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# For fine tuning\n",
    "from peft import LoraConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- Set Up ---------------------- #\n",
    "\n",
    "train_size = 9000 \n",
    "validation_size = 1000 \n",
    "\n",
    "# Downloaded and organized a subset of the patchcamelyon data set (first 10K images) into\n",
    "# a folder called patchcamelyon_subset. Has sub folders \"normal\" and \"tumor\"\n",
    "data = load_dataset(\"./patchcamelyon_subset\", split=\"train\")\n",
    "data = data.train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=validation_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "# rename the 'test' set to 'validation'\n",
    "data[\"validation\"] = data.pop(\"test\")\n",
    "\n",
    "# ------------ Optional: display dataset details ------------ #\n",
    "print(data) \n",
    "# This is actually a dictionary - it contains {'image':blah, 'label':hmmm}\n",
    "print(f\"data['train'][0]: {data['train'][0]}\")\n",
    "# First image in the training data\n",
    "image = data['train'][0]['image']\n",
    "# First label in the training data\n",
    "label = data['train'][0]['label']\n",
    "image.save(\"sample_image.png\")\n",
    "print(\"Image saved to sample_image.png\")\n",
    "print(label)\n",
    "print(data['train'].features['label'])\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "HISTOPATHOLOGY_CLASSES = [\n",
    "    # One option for each class\n",
    "    \"A: no tumor present\",\n",
    "    \"B: tumor present\"\n",
    "]\n",
    "\n",
    "options = \"\\n\".join(HISTOPATHOLOGY_CLASSES)\n",
    "PROMPT = f\"Is a tumor present in this histopathology image?\\n{options}\"\n",
    "\n",
    "# 'example' is the name of the input here - input is a dict.\n",
    "# The key for this dict is a str and the value can be of Any type\n",
    "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
    "    # adds a new entry to the dict\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": PROMPT,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    # label of 0 will map to: (A: no tumor present), label of 1 will map to: (B: tumor present)\n",
    "                    \"text\": HISTOPATHOLOGY_CLASSES[example[\"label\"]],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    # Returns a dict with the same structure - but now {'image':blah, 'label':hmmm, 'message':blumph}\n",
    "    return example\n",
    "\n",
    "data = data.map(format_data)\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 9000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image', 'label'],\n",
    "        num_rows: 1000\n",
    "    })\n",
    "})\n",
    "data['train'][0]: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA3E0>, 'label': 0}\n",
    "Image saved to sample_image.png\n",
    "0\n",
    "ClassLabel(names=['normal', 'tumor'], id=None)\n",
    "Map: 100%|██████████| 9000/9000 [00:00<00:00, 10200.15 examples/s]\n",
    "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5657.86 examples/s]\n",
    "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=96x96 at 0x7F34302CA980>, 'label': 0, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Is a tumor present in this histopathology image?\\nA: no tumor present\\nB: tumor present', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'A: no tumor present', 'type': 'text'}], 'role': 'assistant'}]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Loading Model ---------------------- #\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Check if GPU supports bfloat16\n",
    "if torch.cuda.get_device_capability()[0] < 8:\n",
    "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
    "else: \n",
    "    print('GPU supports bfloat 16. You are good to go :)')\n",
    "\n",
    "# A dictionary of model arguments - ie, 'attn_implementation' maps to 'eager'\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Add a dictionary entry 'quantization_config' - sets the values of 5 parameters in BitsAndBytesConfig() \n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "# This is where .apply_chat_template looks back to\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Use right padding to avoid issues during training\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up For Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------  Set Up for Fine Tuning ---------------------- #\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")\n",
    "   \n",
    "\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        # Applies the chat template from messages and appends that to texts. \n",
    "        # Texts is a list of prompts with both A / B options, and the correct choice A or B.\n",
    "        texts.append(processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ).strip())\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    # Contains 'input_ids'\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    return batch\n",
    "\n",
    "    # The labels are the input_ids, with the padding and image tokens masked in\n",
    "    # the loss computation\n",
    "    #labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first three examples from data['train']\n",
    "data_subset = data['train'].select(range(3))\n",
    "print(type(data_subset))\n",
    "batch = collate_fn(data_subset)\n",
    "#print(f'BATCH: ', batch)\n",
    "a = batch['input_ids'].clone()\n",
    "\n",
    "print(processor.tokenizer.decode([    2,      2,    105,   2364,    109, 255999, 262144, 262144]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "<class 'datasets.arrow_dataset.Dataset'>\n",
    "<bos><bos><start_of_turn>user\n",
    "\n",
    "\n",
    "<start_of_image><image_soft_token><image_soft_token>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch[\"input_ids\"][0].tolist()\n",
    "num_image_tokens = input_ids.count(262144)\n",
    "print(f\"Number of <image_soft_token> tokens: {num_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Number of <image_soft_token> tokens: 256\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each image: 96 × 96 pixels\n",
    "- Number of image tokens: 256  \n",
    "- Therefore:  \n",
    "\n",
    "  $\\frac{96 \\times 96}{256} = 36 \\text{ pixels per patch} \\Rightarrow \\sqrt{36} = 6 \\times 6 \\text{ pixels per patch}$\n",
    "  \n",
    "- The image is divided into 256 patches, each of size 6×6 pixels.  \n",
    "- These patches are flattened, encoded, and each gets represented by one token (`<image_soft_token>`, token ID 262144).  \n",
    "- So:  \n",
    "\n",
    "  $96 \\times 96 \\text{ image} \\quad \\rightarrow \\quad 256 \\text{ tokens} \\quad (\\text{each representing a 6×6 pixel patch})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finding the length of each batch['token_type_ids'][i] \n",
    "batch_ttis = batch['token_type_ids']\n",
    "length = len(batch['token_type_ids'])\n",
    "for i in range(length):\n",
    "    print(len(batch_ttis[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods for dataset.Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(input):\n",
    "    return input\n",
    "\n",
    "# Access rows\n",
    "print(data_subset[0])\n",
    "\n",
    "# Select subset of rows\n",
    "data_subset_small = data_subset.select(range(2))\n",
    "\n",
    "# Apply transformation to all rows\n",
    "data_subset_mapped = data_subset.map(my_func)\n",
    "\n",
    "# Split into train/val\n",
    "split_data = data_subset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Shuffle rows\n",
    "shuffled = data_subset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(batch['input_ids']) == len(batch['attention_mask']):\n",
    "    print('same length')\n",
    "else: print('different length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "same length\n",
    "```\n",
    "\n",
    "This means that the input_ids and the attention_mask have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- scratch work ------------- #\n",
    "# tensor_a = torch.rand(5)\n",
    "# print(tensor_a)\n",
    "# print(len(tensor_a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medgemma_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
